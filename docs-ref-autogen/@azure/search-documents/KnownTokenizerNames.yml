### YamlMime:TSEnum
name: KnownTokenizerNames
uid: '@azure/search-documents.KnownTokenizerNames'
package: '@azure/search-documents'
summary: Defines values for TokenizerName.
fullName: KnownTokenizerNames
fields:
  - name: Classic
    uid: '@azure/search-documents.KnownTokenizerNames.Classic'
    package: '@azure/search-documents'
    summary: >-
      Grammar-based tokenizer that is suitable for processing most
      European-language documents. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html
    numericValue: null
  - name: EdgeNGram
    uid: '@azure/search-documents.KnownTokenizerNames.EdgeNGram'
    package: '@azure/search-documents'
    summary: >-
      Tokenizes the input from an edge into n-grams of the given size(s). See

      https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html
    numericValue: null
  - name: Keyword
    uid: '@azure/search-documents.KnownTokenizerNames.Keyword'
    package: '@azure/search-documents'
    summary: >-
      Emits the entire input as a single token. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html
    numericValue: null
  - name: Letter
    uid: '@azure/search-documents.KnownTokenizerNames.Letter'
    package: '@azure/search-documents'
    summary: >-
      Divides text at non-letters. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html
    numericValue: null
  - name: Lowercase
    uid: '@azure/search-documents.KnownTokenizerNames.Lowercase'
    package: '@azure/search-documents'
    summary: >-
      Divides text at non-letters and converts them to lower case. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html
    numericValue: null
  - name: MicrosoftLanguageStemmingTokenizer
    uid: >-
      @azure/search-documents.KnownTokenizerNames.MicrosoftLanguageStemmingTokenizer
    package: '@azure/search-documents'
    summary: >-
      Divides text using language-specific rules and reduces words to their base
      forms.
    numericValue: null
  - name: MicrosoftLanguageTokenizer
    uid: '@azure/search-documents.KnownTokenizerNames.MicrosoftLanguageTokenizer'
    package: '@azure/search-documents'
    summary: Divides text using language-specific rules.
    numericValue: null
  - name: NGram
    uid: '@azure/search-documents.KnownTokenizerNames.NGram'
    package: '@azure/search-documents'
    summary: >-
      Tokenizes the input into n-grams of the given size(s). See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html
    numericValue: null
  - name: PathHierarchy
    uid: '@azure/search-documents.KnownTokenizerNames.PathHierarchy'
    package: '@azure/search-documents'
    summary: >-
      Tokenizer for path-like hierarchies. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html
    numericValue: null
  - name: Pattern
    uid: '@azure/search-documents.KnownTokenizerNames.Pattern'
    package: '@azure/search-documents'
    summary: >-
      Tokenizer that uses regex pattern matching to construct distinct tokens.
      See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html
    numericValue: null
  - name: Standard
    uid: '@azure/search-documents.KnownTokenizerNames.Standard'
    package: '@azure/search-documents'
    summary: >-
      Standard Lucene analyzer; Composed of the standard tokenizer, lowercase
      filter and stop

      filter. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html
    numericValue: null
  - name: UaxUrlEmail
    uid: '@azure/search-documents.KnownTokenizerNames.UaxUrlEmail'
    package: '@azure/search-documents'
    summary: >-
      Tokenizes urls and emails as one token. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html
    numericValue: null
  - name: Whitespace
    uid: '@azure/search-documents.KnownTokenizerNames.Whitespace'
    package: '@azure/search-documents'
    summary: >-
      Divides text at whitespace. See

      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html
    numericValue: null
